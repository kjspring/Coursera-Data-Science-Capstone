---
title: "Data Science Capstone Task 02"
author: "Kevin Spring"
date: "11/12/2014"
output: html_document
---

## Task 2 - Exploratory analysis

```{r}
# load packages
library(RWeka)
library(tm)
library(wordcloud)
library(ggplot2)
library(qdap)

data('crude')
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm2 <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
inspect(tdm[340:345,1:10])

```
### Tasks to accomplish

1. Exploratory analysis - perform a through exploratory analysis of the data, understanding the distribution of wrods and relationship between the words in the corpus.
2. Understanding frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of word and word pairs in the data

### Questions to consider
1. Some words are more frequent than others - what are the distributions of word frequencies?
```{r}
freq <- rowSums(as.matrix(sparse)) # get the total frequencies of each stemmed word
ord_freq <- order(freq) # list the most/least frequent terms
freq[head(ord_freq)] # most infrequent words
freq[tail(ord_freq)] # most frequent words
```
2. What are the frequencies of 2-gram and 3-gram in the dataset?
3. How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90?
4. How do you evaluate how many of the words come from foreign languages?
5. Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover teh same number of phrases?

```{r}
# Figures

# Figure 1: Comparison of Stop word removal versus not removed wordclouds

# calculate the word frequency
wordcloud_creator <- function(m) {
    v <- sort(rowSums(m), decreasing=TRUE)
    myNames <- names(v)
    k <- which(names(v)=="miners")
    myNames[k] <- "mining"
    d <- data.frame(words=myNames, freq=v)
    return(wordcloud(d$word, d$freq, min.freq=10000, rot.per=0.2, max.words=100, scale=c(5, .1), colors=brewer.pal(6, 'Dark2')))
    }
corpus_m <- as.matrix(sparse)
write.csv(corpus_m, file='Task1/output/tdm.csv')
corpus_nostop_m <- as.matrix(sparse_nostop)
write.csv(corpus_nostop_m, file='Task1/output/tdm_nostop.csv')

wordcloud_creator(corpus_m)
wordcloud_creator(corpus_nostop_m)
```

```{r}
# Figure 2: Correlation plot

# Note: does not work
plot(sparse, terms=findFreqTerms(tdm, lowfreq=100)[1:5], corThreshold=0.9)
plot(sparse_nostop)
```

```{r}
# Figure 3: Frequency plot
library(ggplot2)
freq <- sort(rowSums(as.matrix(sparse_nostop)), decreasing=T)
head(freq)
word_freq <- data.frame(word=names(freq), freq=freq)
head(word_freq)

wf <- subset(word_freq, freq>10000)
ggplot(wf, aes(word, freq)) + 
    geom_bar(stat="identity") + 
    theme(axis.text.x=element_text(angle=45, hjust=1)) 
    #scale_fill_gradient("Count", low = "green", high = "red")
```

## Quantitative analysis of text

```{r}

words <- tdm

```

# Figure 4: Word length counts

```{r}

```