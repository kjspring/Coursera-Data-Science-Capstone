---
title: "Data Science MIlestone Report"
author: "Kevin Spring"
date: "11/15/2014"
output: html_document
---

```{r, echo=FALSE, cache=TRUE}
# R code do not show and cache

# Load libraries
library(RWeka)
library(tm)
library(SnowballC)
library(ggplot2)
library(wordcloud)

## Load the training data
twitter <- readLines('Task1/output/twitter.txt')
news <- readLines('Task1/output/news.txt')
blogs <- readLines('Task1/output/blogs.txt')

# remove non-English characters
twitter <- iconv(twitter, "latin1", "ASCII", sub="")
news <- iconv(news, "latin1", "ASCII", sub="")
blogs <- iconv(blogs, "latin1", "ASCII", sub="")

# create the tm corpus object
myCorpus <- Corpus(VectorSource(c(twitter, blogs, news))) # create the corpus

# Data cleaning
# set all words to lowercase
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove profanity
badWordList <- source('files/BadWordList.R') # load Googles bad word list
myCorpus <- tm_map(myCorpus, removeWords, badWordList$value)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)

# remove stopwords, save as new corpus
myCorpus_nostop <- tm_map(myCorpus, removeWords, stopwords('english'))
myCorpus_nostop <- tm_map(myCorpus_nostop, removeWords, c('like','the')) # remove like as a stopword

# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus_nostop <- tm_map(myCorpus_nostop, stripWhitespace)

# Create the term document matrix
tdm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 2))
tdm_nostop <- TermDocumentMatrix(myCorpus_nostop, control = list(minWordLength = 2))

# Remove sparse terms (things like "aaaaaannnnndddddwer" are removed)
sparse = removeSparseTerms(tdm, 0.999)
sparse_nostop = removeSparseTerms(tdm_nostop, 0.999)

options(mc.cores=1)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
#TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))
bigramTDM <- TermDocumentMatrix(myCorpus_nostop, control=list(tokenize = BigramTokenizer))
# remove sparse terms
bigramTDM <- removeSparseTerms(bigramTDM, 0.999)
#options(mc.cores=4)
#trigramTDM <- TermDocumentMatrix(myCorpus_nostop, control=list(tokenize = TrigramTokenizer))
#a <- NGramTokenizer(corpus, Weka_control(min = 2, max = 3, delimiters = " \\r\\n\\t"))

# remove excess objects
#rm(c(twitter, blogs, myCorpus, myCorpus_nostop, news, tdm, tdm_nostop, badWordList ))
```

## Introduction

The purpose of this project is to predict a word a user is typing and also predict a successive word after a word is typed. The final product will be a web application that allows a user to input text into a text field. The application will predict what word they are typing and display a few words that are predicted. The user can click on these words for faster user input if the words are correctly predicted or continue to type. As the user continues to input characters the application will continue to display predicted words on the sequence of character inputs. After a word is inputted or selected by the user, the application will predict what next word the user will input and display a list of possible predicted words or phrases. The user can select the application predicted text or continue to input characters.

## Data Manipulation

The data used in this analysis is text from english language twitter feeds, blogs, and news. The twitter, news, and blog text data has 2,360,148, 1,010,242, and 899,288 lines of code, respectively.

The training data set was limited to 50,000, 20,000, and 20,000 lines of code for the twitter, news, and blog text data, respectively. This training set was selected by taking a random sample of each of the twitter, news, and blog text data sets. Before we can accuratly assess how many words are in each line of text the data needs to be cleaned. I used the R programming language with the `tm` package to analyze the text data.

## Results

The first step is to remove any non-Latin characters in the data set. Althought this data is classified as English, there is non-Latin characters in the data. The characters removed includes emoticons, Chinese hanzi, and Japanese kana. Arabic numbers are also removed from the training data. There were a total of 85521 terms in the combined twitter, news, and blog text data. 

A further reduction of words such as onomatopoeias and profanity filters the data set to 2108 terms. Profanity is commonly used in the corpus but serves no prediction value to this project. The same profanity word can be used as a verb, noun, adverb, or adjective and thus prediction is limited for these words. As a user interface tool, showing profanity for predicted words if the user did not want to see may cause the user to use a competitor's product. If the user uses profanity then those words can be added to the dictionary of the final tool to accurately predict and suit the user's language style.

As shown in Table 1, the ten most common terms are stop words. Table 2 shows the ten most common words with stop words removed from the corpus. Unlike most test data analysis, I will be keeping stop words in the corpus. Term frequency was measured using the number of times a term is used in a document.

```{r, echo=FALSE}
# Basic data tables
freq <- sort(rowSums(as.matrix(sparse)), decreasing=TRUE) # get the total frequencies of each stemmed word
table1 <- data.frame(word=names(freq), freq=freq)
rm(freq)
freq2 <- sort(rowSums(as.matrix(sparse_nostop)), decreasing=TRUE)
table2 <- data.frame(word=names(freq2), freq=freq2)
rm(freq2)
bigram_freq <- sort(rowSums(as.matrix(bigramTDM)), decreasing=TRUE)
table_bigram <- data.frame(word=names(bigram_freq), freq=bigram_freq)
```


```{r, echo=FALSE}
# Figure 1: word cloud
wordcloud_creator <- function(m) {
    v <- sort(rowSums(m), decreasing=TRUE)
    myNames <- names(v)
    k <- which(names(v)=="miners")
    myNames[k] <- "mining"
    d <- data.frame(words=myNames, freq=v)
    return(wordcloud(d$word, d$freq, min.freq=10000, rot.per=0.2, max.words=100, scale=c(5, .1), colors=brewer.pal(6, 'Dark2')))
    }
corpus_m <- as.matrix(sparse)
#write.csv(corpus_m, file='Task1/output/tdm.csv')
corpus_nostop_m <- as.matrix(sparse_nostop)
#write.csv(corpus_nostop_m, file='Task1/output/tdm_nostop.csv')

wordcloud_creator(corpus_m)
wordcloud_creator(corpus_nostop_m)
```


```{r, echo=FALSE}
# histogram to illustrate features of the data
wf <- subset(head(table1, 10))
wf_nostop <- subset(head(table2, 10))

histogram <- ggplot(data=wf, aes(word, freq))
histogram + 
    geom_bar(stat="identity") +
    theme(axis.text.x=element_text(angle=45, hjust=1))

histogram <- ggplot(data=wf_nostop, aes(word, freq))
histogram + 
    geom_bar(stat="identity") +
    theme(axis.text.x=element_text(angle=45, hjust=1))
```

### N-gram tokenization
A bigram is two words that appear in a sequence. For example, "happy birthday" would be a bigram. Table 3 shows the top ten bigrams in the corpus after sparse terms were removed.

### Stop word removal
Stop words are common words used in languages such as function words such as, the, is, at, which, and on. While it is important to accurately predict these words in this project, the inclusion of these words in the main prediction algorithm would over estimate the user wanting to type these words versus other words. See Fig X to compare the most common words with and without stop word removal. A second prediction algorithm can be used to accurately predict stop word use on a bigram model.


### Correlated terms
If two words always appear together then the correlation would be 1.0. If they never appear together the correlation would be 0. This correlation is a measurement of how closely associated the words are in the corpus.

## Conclusions
