f` <- function(x) {
4*x^3 - 6*x^2 + 1
}
`
g <- function(x) {
4*x^3 - 6*x^2 + 1
}
plot(x, g(x))
f(1.4)
f(1.5)
f(1.3)
f(1.45)
f(1.35)
f(1.32)
f(1.36)
f(1.37)
f(1.38)
3*16-24
(-1)^3
(-1)^(1/3)
(1)^(1/3)
g <- function(x) {
x^(1/3)
}
x <- runif(100, -1, 1)
plot(x, g(x))
h <- function(t) {
t/(t-3)
}
t <- runif(100, 4, 5)
plot(t, h(t))
f <- function(x) {
if(x >= 0 & x <= 1) {2*x+4}
if(x>1 & x <= 3) {6*x^2}
}
x <- runif(100, 0, 3)
plot(x, f(x))
f <- function(x) {
if(x >= 0 && x <= 1) {2*x+4}
if(x>1 && x <= 3) {6*x^2}
}
x <- runif(100, 0, 3)
plot(x, f(x))
f(1)
f(1)
f
f <- function(x) {
if(x >= 0 && x <= 1) {return 2*x+4}
if(x>1 && x <= 3) {return 6*x^2}
}
x <- runif(100, 0, 3)
plot(x, f(x))
?apply
f(x)
?sapply
sapply(x, f)
f <- function(x) {
y <- vector()
for(i in 1:length(x))
if(x >= 0 && x <= 1) {
y[x] <- 2*x+4
}
if(x>1 && x <= 3) {
y[x] <- 6*x^2
}
}
x <- runif(100, 0, 3)
f <- function(x) {
y <- vector()
for(i in 1:length(x))
if(x >= 0 && x <= 1) {
y[x] <- 2*x+4
}
if(x>1 && x <= 3) {
y[x] <- 6*x^2
}
return y
}
f <- function(x) {
y <- vector()
for(i in 1:length(x))
if(x >= 0 && x <= 1) {
y[x] <- 2*x+4
}
if(x>1 && x <= 3) {
y[x] <- 6*x^2
}
}
return y
}
f <- function(x) {
y <- vector()
for(i in 1:length(x)) {
if(x >= 0 && x <= 1) {
y[x] <- 2*x+4
}
if(x>1 && x <= 3) {
y[x] <- 6*x^2
}
}
return y
}
f <- function(x) {
y <- vector()
for(i in 1:length(x)) {
if(x >= 0 && x <= 1) {
y[x] <- 2*x+4
}
if(x>1 && x <= 3) {
y[x] <- 6*x^2
}
}
return(y)
}
f(1)
f(2)
f <- function(x) {
y <- vector()
for(i in 1:length(x)) {
if(x >= 0 && x <= 1) {
y[i] <- 2*x+4
}
if(x>1 && x <= 3) {
y[i] <- 6*x^2
}
}
return(y)
}
f(2)
f(x)
plot(x, f(x))
f(1)
f(2)
f(3)
f(4)
f(5)
f(6)
f(0)
x
f(0)
f(.5)
length(x)
f <- function(x) {
y <- vector()
for(i in 1:length(x)) {
if(x[i] >= 0 && x[i] <= 1) {
y[i] <- 2*x+4
}
if(x[i]>1 && x[i] <= 3) {
y[i] <- 6*x^2
}
}
return(y)
}
x <- runif(100, 0, 3)
plot(x, f(x))
f(3)
f(4)
f(2.5)
a <- f(x)
a
?seq
x <- seq(from=0, to=3, by=.1)
f(x)
x
length(x)
f(x)
warnings()
f <- function(x) {
y <- vector()
for(i in 1:length(x)) {
if(x[i] >= 0 && x[i] <= 1) {
y[i] <- 2*x[i]+4
}
if(x[i]>1 && x[i] <= 3) {
y[i] <- 6*x[i]^2
}
}
return(y)
}
f(x)
plot(x, f(x))
f(0)
f(3)
sqrt(3)*sqrt(12)
?tan
atan(-sqrt(3)/3)
f <- function(x) {
x^2 - 8*x - 9
}
x <- runif(100, -10, 10)
plot(x, f(x))
f <- function(x) {
x*sqrt(x-7)
}
plot(x, f(x))
f <- function(x) {
(3*x+14) / (2*sqrt(x+17))
}
plot(x, f(x))
f(1)
f(0)
f(-1)
f(-5)
f(-5.1)
f(-4.5)
f(4)
f(-4)
f(-3)
f(-3.5)
f(-4.2)
f(-4.1)
f(-4.3)
f(-4.4)
f(-4.5)
f(-4.6)
f(-4.7)
f(-4.8)
f(-4.75)
sqrt(0)
-14/3
f(-14/3)
-48/-9
48/3
.5^2 + 4.5
f <- function(x) {
x^2 - 9x - 13
}
plot(x, f(x))
f <- function(x) {
x^2 - 9x - 13
}
plot(x, f(x))
f <- function(x) {
x^2 - 9*x - 13
}
plot(x, f(x))
x <- runif(100, 1, 6)
plot(x, f(x))
x <- runif(100, 0, 6)
plot(x, f(x))
x <- runif(100, -6, 6)
plot(x, f(x))
x <- runif(100, -2, 6)
plot(x, f(x))
-8^(2/3)
8^(2/3)
4-3
-8^(2/3)
(-8^(2/3))
(-8)^1/3
((-8)^(1/3))^2
(-8)^(1/3)
64^(1/3)
-84/3
84/3
6^3-9*6^2+6
x <- runif(100, -20, 20)
f <- function(x) {
x^(1/3) * (x^2 - 63)
}
x <- runif(100, -20, 20)
plot(x, f(x))
f <- function(x) {
x^(1/3)*(x^2-112)
}
x <- runif(100, -5, 5)
plot(x, f(x))
f(4)
-20/3
f <- function(x) {
x^(1/3)*(x^2-112)
}
x <- runif(100, -100, 100)
plot(x, f(x))
mean(8,9,10,11,12)
a <- c(8,9,10,11,12)
mean(a)
var(a)
sd(a)
sqrt(2.5)
1 - 0.05
file_location <- '~/Dropbox/courses/coursera/DataScience/10_Capstone/data/final/en_US/en_US.twitter.txt'
twitter <- scan.lines(file_location)
twitter <- scan(file_location)
?scan
length(readLines(file_location))
twitter <- readLines(file_location)
news <- readLines(news_location)
blog_location <- '~/Dropbox/courses/coursera/DataScience/10_Capstone/data/final/en_US/en_US.blog.txt'
news_location <- '~/Dropbox/courses/coursera/DataScience/10_Capstone/data/final/en_US/en_US.news.txt'
news <- readLines(news_location)
length(twitter)
str(news)
string <- "A computer once beat me at chess, but it was no match for me at kickboxing"
q6 <- grepl(string, twitter)
length(q6)
q6[1:10]
length(which(q6==TRUE))
ch_string <- 'biostats'
q5 <- grepl(ch_string, twitter)
which(q5 == TRUE)
twitter[which(q5 == TRUE)]
?grepl
love_string <- 'love'
hate_string <- 'hate'
love <- twitter[which(grepl(love_string, twitter))]
length(love)
hate <- twitter[which(grepl(hate_string, twitter))]
length(hate)
length(love)/length(hate)
nchar(twitter)
max(twitter)
twitter_max <- nchar(max(twitter))
news_max <- nchar(max(news))
blog_max <- nchar(max(glob))
blog_max <- nchar(max(blog))
blog <_ readLines(blog_location)
blog <- readLines(blog_location)
blog_location <- '~/Dropbox/courses/coursera/DataScience/10_Capstone/data/final/en_US/en_US.blogs.txt'
blogs <- readLines(blog_location)
blog_max <- nchar(max(blogs))
blog_max
news_max
(max(news)
)
max(blogs)
length(blogs)
sub_blog <- blog[1:25]
sub_blogs <- blogs[1:25]
sub_blogs
max(sub_blogs)
nchar(sub_blogs)
max(nchar(twitter))
max(nchar(news))
max(nchar(blogs))
48/9
48/3
sqrt(16/3+8)
-9*16/3+48
setwd("~/projects/data_science_capstone")
# load the random lines
twitter <- readLines('Task1/output/twitter.txt')
news <- readLines('Task1/output/news.txt')
blogs <- readLines('Task1/output/blogs.txt')
library(RWeka)
library(tm)
```
twitter <- sapply(twitter, function(row) iconv(row, "latin1", "ASCII", sub=""))
news <- sapply(news, function(row) iconv(row, "latin1", "ASCII", sub=""))
blogs <- sapply(twitter, function(row) iconv(row, "latin1", "ASCII", sub=""))
myCorpus <- Corpus(VectorSource(c(twitter, blogs, news))) # create the corpus
# set all words to lowercase
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove profanity
badWordList <- source('files/BadWordList.R') # load Googles bad word list
myCorpus <- tm_map(myCorpus, removeWords, badWordList$value)
# remove stopwords
myCorpus <- tm_map(myCorpus, removeWords, stopwords('english'))
twitter_location <- '~/Dropbox/courses/coursera/DataScience/10_Capstone/data/final/en_US/en_US.twitter.txt'
blog_location <- '~/Dropbox/courses/coursera/DataScience/10_Capstone/data/final/en_US/en_US.blogs.txt'
news_location <- '~/Dropbox/courses/coursera/DataScience/10_Capstone/data/final/en_US/en_US.news.txt'
twitter <- readLines(twitter_location)
news <- readLines(news_location)
blogs <- readLines(blog_location)
random_twitter <- ceiling(runif(n=50000, min=0, max=length(twitter)))
random_news <- ceiling(runif(n=20000, min=0, max=length(news)))
random_blogs <- ceiling(runif(n=20000, min=0, max=length(blogs)))
twitter <- twitter[random_twitter]
news <- news[random_news]
blogs <- blogs[random_blogs]
length(news)
length(blogs)
length(twitter)
write.table(twitter, file='Task1/output/twitter.txt', row.names=FALSE, col.names=FALSE)
write.table(news, file='Task1/output/news.txt', row.names=FALSE, col.names=FALSE)
write.table(blogs, file='Task1/output/blogs.txt', row.names=FALSE, col.names=FALSE)
twitter <- readLines('Task1/output/twitter.txt')
news <- readLines('Task1/output/news.txt')
blogs <- readLines('Task1/output/blogs.txt')
twitter <- sapply(twitter, function(row) iconv(row, "latin1", "ASCII", sub=""))
news <- sapply(news, function(row) iconv(row, "latin1", "ASCII", sub=""))
blogs <- sapply(twitter, function(row) iconv(row, "latin1", "ASCII", sub=""))
twitter <- readLines('Task1/output/twitter.txt')
news <- readLines('Task1/output/news.txt')
blogs <- readLines('Task1/output/blogs.txt')
twitter <- sapply(twitter, function(row) iconv(row, "latin1", "ASCII", sub=""))
news <- sapply(news, function(row) iconv(row, "latin1", "ASCII", sub=""))
blogs <- sapply(twitter, function(row) iconv(row, "latin1", "ASCII", sub=""))
myCorpus <- Corpus(VectorSource(c(twitter, blogs, news))) # create the corpus
# set all words to lowercase
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove profanity
badWordList <- source('files/BadWordList.R') # load Googles bad word list
myCorpus <- tm_map(myCorpus, removeWords, badWordList$value)
# remove stopwords
myCorpus <- tm_map(myCorpus, removeWords, stopwords('english'))
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
str(twitter)
inspect(myCorpus[1])
tdm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 2,
weighting=
function(x) weightTfIdf(x, normalize=FALSE)))
findFreqTerms(tdm, 1000)
inspect(removeSparseTerms(dtm, .001))
m <- as.matrix(tdm)
str(tdm)
crude
data(crude)
crude
?removeWords
controls <- list(
content_transformer(tolower) = TRUE,
removePunctuation = TRUE,
stopwords = stopwords("english"),
removeWords(words = badWordList$value)
stemming = function(word) wordStem(word, language = "english")
)
controls <- list(
content_transformer(tolower) = TRUE,
removePunctuation = TRUE,
stopwords = stopwords("english"),
removeWords(words = badWordList$value),
stemming = function(word) wordStem(word, language = "english")
)
sparse = as.data.frame(as.matrix(removeSparseTerms(dtm, 0.8)))
sparse = as.data.frame(as.matrix(removeSparseTerms(tdm, 0.8)))
sparse[1,1]
sparse[1,2]
sparse
sparse[1:200, 1:50]
myCorpus <- tm_map(myCorpus, stemDocument, language='english')
library(Rstem)
install.packages('Rstem')
library(Rstem)
isntall.packages('rstem')
install.packages('rstem')
myCorpus <- tm_map(myCorpus, function(word) wordStem(word, language = "english"))
myCorpus <- tm_map(myCorpus, stemDocument, language='english')
sparse
inspect(sparse[1])
sparse = removeSparseTerms(tdm, 0.8)
str(sparse)
sparse = removeSparseTerms(tdm, 0.2)
str(sparse)
str(tdm)
inspect(tdm[1:5,])
inspect(tdm[,1:5])
?inspect
inspect(tdm)
tdm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 2,
weighting=
function(x) weightTfIdf(x, normalize=FALSE)))
tdm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 2))
myCorpus <- Corpus(VectorSource(c(twitter, blogs, news))) # create the corpus
# set all words to lowercase
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove profanity
badWordList <- source('files/BadWordList.R') # load Googles bad word list
myCorpus <- tm_map(myCorpus, removeWords, badWordList$value)
# remove stopwords
myCorpus <- tm_map(myCorpus, removeWords, stopwords('english'))
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
tdm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 2))
inspect(tdm)
twitter <- readLines('Task1/output/twitter.txt')
news <- readLines('Task1/output/news.txt')
blogs <- readLines('Task1/output/blogs.txt')
twitter <- sapply(twitter, function(row) iconv(row, "latin1", "ASCII", sub=""))
twitter[1:3]
colnames(twitter)
names(twitter)
names(twitter) <- NULL
names(twitter)[1:3]
names(twitter) <- "twitter"
names(blogs) <- "blogs"
names(news) <- "news"
myCorpus <- Corpus(VectorSource(c(twitter, blogs, news))) # create the corpus
# set all words to lowercase
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove profanity
badWordList <- source('files/BadWordList.R') # load Googles bad word list
myCorpus <- tm_map(myCorpus, removeWords, badWordList$value)
# remove stopwords
myCorpus <- tm_map(myCorpus, removeWords, stopwords('english'))
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
tdm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 2))
inspect(tdm)
inspect(tdm[340:345,1:10])
sparse = removeSparseTerms(tdm, 0.2)
inspect(sparse)
inspect(sparse[340:345,1:10])
inspect(sparse[1:2,1:10])
inspect(sparse[1:2,1:2])
str(sparse)
dtm <- DocumentTermMatrix(myCorpus, control=list(minWordLength = 2))
inspect(dtm)
